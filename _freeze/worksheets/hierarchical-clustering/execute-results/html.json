{
  "hash": "b1a5a759e1f086ee04ef71cda764f480",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Hierarchical clustering\"\nauthor: \"Claus O. Wilke\"\nformat: live-html\nengine: knitr\nwebr:\n  render-df: default\n---\n\n::: {.cell}\n\n:::\n\n\n\n\n## Introduction\n\nIn this worksheet, we will discuss how to perform hierarchical clustering.\n\nFirst we need to load the required R packages. Please wait a moment until the live R session is fully set up and all packages are loaded.\n\n\n\n::: {.cell edit='false'}\n```{webr}\n#| warning: false\n#| edit: false\nlibrary(tidyverse)\nlibrary(ggdendro)\nlibrary(palmerpenguins)\n```\n:::\n\n\n\nNext we set up the data. It's a downsampled and slightly modified version of the `penguins` dataset from the package **palmerpenguins.**\n\n\n\n::: {.cell edit='false'}\n```{webr}\n#| edit: false\n#| warning: false\npenguin_data <- penguins |>\n  select(-island, -sex, -year) |>\n  group_by(species) |>\n  mutate(\n    id = 1:n(),\n    penguin = glue::glue(\"{species}-{id}\")\n  ) |>\n  na.omit() |>\n  ungroup() |>\n  select(-id, -species)\n\n# down-sample\nset.seed(456321)\npenguins_sampled <- penguin_data[sample(1:nrow(penguin_data), 30), ] |>\n  arrange(penguin) |>\n  column_to_rownames(var = \"penguin\")\n\npenguins_sampled\n```\n:::\n\n\n\n## Calculating a distance matrix\n\nMany hierarchical clustering algorithms start with calculating a distance matrix. This is done with the built-in R function `dist()`. Before calculating distances, though, you should first scale the data to zero mean and unit variance, just like we have done previously for PCA and other multivariate techniques. As a reminder, you scale a dataset with the function `scale()`.\n\nTry this out on the `penguins_sampled` dataset. Scale the data and then calculate the distance matrix.\n\n\n\n::: {.cell exercise='distance_calc'}\n```{webr}\n#| exercise: distance_calc\npenguins_sampled |>\n  ___ |>\n  ___\n```\n:::\n\n\n\n::: { .hint exercise=\"distance_calc\" }\n::: { .callout-tip title=\"Hint\" collapse=\"false\"}\n```r\npenguins_sampled |>\n  scale() |>\n  ___\n```\n:::\n:::\n\n::: { .solution exercise=\"distance_calc\" }\n::: { .callout-tip title=\"Solution\" collapse=\"false\"}\n```r\npenguins_sampled |>\n  scale() |>\n  dist()\n```\n:::\n:::\n\nBy default, the `dist()` function calculates Euclidean distances. But other options exist, which can be selected via the `method` argument to the `dist()` function. Commonly used options include `\"maximum\"`, `\"manhattan\"`, or `\"minkowski\"`. Try this out.\n\n\n\n::: {.cell exercise='distance-calc-methods'}\n```{webr}\n#| exercise: distance-calc-methods\n\n```\n:::\n\n\n\n::: { .hint exercise=\"distance-calc-methods\" }\n::: { .callout-tip title=\"Hint\" collapse=\"false\"}\n```r\npenguins_sampled |>\n  scale() |>\n  dist(method = ___)\n```\n:::\n:::\n\n::: { .solution exercise=\"distance-calc-methods\" }\n::: { .callout-tip title=\"Solution\" collapse=\"false\"}\n```r\npenguins_sampled |>\n  scale() |>\n  dist(method = \"maximum\")\n\n# also try \"manhattan\" and \"minkowski\"\n```\n:::\n:::\n\nWhen using the Minkowski distance, you should also set the parameter `p`. The Minkowski distance is a generalization of both the Euclidean and the Manhattan distance, and the parameter `p` interpolates between these distance types.\n\nVerify that the Minkowski distance is identical to the Euclidean distance for `p = 2` and identical to the Manhattan distance for `p = 1`. The simplest way to do this is to calculate the two distance matrices and then subtract them from each other and check that the values are 0.\n\n\n\n::: {.cell exercise='distance-calc-minkowski'}\n```{webr}\n#| exercise: distance-calc-minkowski\n\n```\n:::\n\n\n\n::: { .hint exercise=\"distance-calc-minkowski\" }\n::: { .callout-tip title=\"Hint\" collapse=\"false\"}\n```r\n# calculate distances\nd_eucl <- penguins_sampled |>\n  scale() |>\n  dist(method = \"euclidean\")\n\nd_mink <- penguins_sampled |>\n  scale() |>\n  dist(method = \"minkowski\", p = 2)\n\n# then subtract to check for equality\n```\n:::\n:::\n\n::: { .solution exercise=\"distance-calc-minkowski\" }\n::: { .callout-tip title=\"Solution\" collapse=\"false\"}\n```r\n# calculate distances\nd_eucl <- penguins_sampled |>\n  scale() |>\n  dist(method = \"euclidean\")\n\nd_mink <- penguins_sampled |>\n  scale() |>\n  dist(method = \"minkowski\", p = 2)\n\n# then subtract to check for equality\nd_eucl - d_mink\n```\n:::\n:::\n\n## Performing hierarchical clustering\n\nTo perform hierarchical clustering, you simply run the function `hclust()` on a distance matrix previously computed with `dist()`. You can then visualize the result with `ggdendrogram()` from the **ggdendro** package. Try this out. (Hint: Write one consecutive sequence of pipe commands.)\n\n\n\n::: {.cell exercise='ggdendro-simple'}\n```{webr}\n#| exercise: ggdendro-simple\npenguins_sampled |>\n  scale() |>\n  dist() |>\n  ___\n```\n:::\n\n\n\n::: { .hint exercise=\"ggdendro-simple\" }\n::: { .callout-tip title=\"Hint\" collapse=\"false\"}\n```r\npenguins_sampled |>\n  scale() |>\n  dist() |>\n  hclust() |>\n  ___\n```\n:::\n:::\n\n::: { .solution exercise=\"ggdendro-simple\" }\n::: { .callout-tip title=\"Solution\" collapse=\"false\"}\n```r\npenguins_sampled |>\n  scale() |>\n  dist() |>\n  hclust() |>\n  ggdendrogram()\n```\n:::\n:::\n\nIn the `ggdendrogram()` function, you can set `rotate = TRUE` to arrange the leaves of the dendrogram vertically instead of horizontally. Try this out.\n\n\n\n::: {.cell exercise='ggdendro-rotated'}\n```{webr}\n#| exercise: ggdendro-rotated\n\n```\n:::\n\n\n\n::: { .hint exercise=\"ggdendro-rotated\" }\n::: { .callout-tip title=\"Hint\" collapse=\"false\"}\n```r\npenguins_sampled |>\n  scale() |>\n  dist() |>\n  hclust() |>\n  ggdendrogram(rotate = ___)\n```\n:::\n:::\n\n::: { .solution exercise=\"ggdendro-rotated\" }\n::: { .callout-tip title=\"Solution\" collapse=\"false\"}\n```r\npenguins_sampled |>\n  scale() |>\n  dist() |>\n  hclust() |>\n  ggdendrogram(rotate = TRUE)\n```\n:::\n:::\n\nYou can run different clustering algorithms by changing the `method` argument of `hclust()`. `method = \"average\"` uses UPGMA, `method = \"ward.D2\"` uses Ward's minimum variance method, and `method = \"complete\"` uses the complete linkage method. Modify the example above to try these different methods.\n\n## Assigning observations to clusters\n\nIn hierarchical clustering, if we want to assign each observation to a cluster, we need to cut the dendrogram into disjoint parts. There are two ways in which we can do this. First, we can cut such that we obtain a specific number of clusters. Second, we can cut at a set height and see how many clusters we obtain.\n\nWe can cut a dendrogram with the function `cutree()`, which takes as input the output from `hclust()` and either an argument `k` to determine how many clusters we want or an argument `h` to determine at which height we want to cut the tree. Let's try the first approach first. Cut the penguin dendrogram such that there are three clusters. Then check whether the three clusters correspond to the three species.\n\n\n\n::: {.cell exercise='cutree-k'}\n```{webr}\n#| exercise: cutree-k\npenguins_sampled |>\n  scale() |>\n  dist() |>\n  hclust() |>\n  ___\n```\n:::\n\n\n\n::: { .hint exercise=\"cutree-k\" }\n::: { .callout-tip title=\"Hint\" collapse=\"false\"}\n```r\npenguins_sampled |>\n  scale() |>\n  dist() |>\n  hclust() |>\n  cutree(k = ___)\n```\n:::\n:::\n\n::: { .solution exercise=\"cutree-k\" }\n::: { .callout-tip title=\"Solution\" collapse=\"false\"}\n```r\npenguins_sampled |>\n  scale() |>\n  dist() |>\n  hclust() |>\n  cutree(k = 3)\n```\n:::\n:::\n\nNext, by trial-and-error, find a cut height at which you obtain exactly three clusters.\n\n\n\n\n::: {.cell exercise='cutree-h'}\n```{webr}\n#| exercise: cutree-h\n```\n:::\n\n\n\n::: { .hint exercise=\"cutree-h\" }\n::: { .callout-tip title=\"Hint\" collapse=\"false\"}\n```r\npenguins_sampled |>\n  scale() |>\n  dist() |>\n  hclust() |>\n  cutree(h = ___)\n```\n:::\n:::\n\n::: { .solution exercise=\"cutree-h\" }\n::: { .callout-tip title=\"Solution\" collapse=\"false\"}\n```r\npenguins_sampled |>\n  scale() |>\n  dist() |>\n  hclust() |>\n  cutree(h = 2.9)\n```\n:::\n:::\n\nCould you have used the function `ggdendrogram()` to arrive at a good guess for the value of `h`?\n\nFinally, try different distance methods and see whether the clusters continue to match species identity when you cut into `k = 3` clusters. Can you find a distance metric for which the clusters do not match the species?\n\n\n\n::: {.cell exercise='cutree-dist'}\n```{webr}\n#| exercise: cutree-dist\n\n```\n:::\n\n\n\n::: { .solution exercise=\"cutree-dist\" }\n::: { .callout-tip title=\"Solution\" collapse=\"false\"}\n```r\n# for Manhattan distance, Adelie and Chinstrap are mixed together\npenguins_sampled |>\n  scale() |>\n  dist(method = \"manhattan\") |>\n  hclust() |>\n  cutree(k = 3)\n```\n:::\n:::\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}